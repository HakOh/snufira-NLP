{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['and', 'word2vec', 'for', 'sentence', 'this', 'is', 'one', 'second', 'another', 'the', 'first', 'yet', 'final', 'more']\n",
      "[  1.09108200e-03  -1.50774524e-03  -3.20697599e-03  -9.94841801e-04\n",
      "   3.15078214e-04   4.37214086e-03  -3.69665888e-03  -3.33983405e-03\n",
      "  -4.54258965e-03   9.99633805e-04  -4.23612585e-03  -3.83223081e-03\n",
      "   4.32860339e-03  -3.25149111e-03   2.95695267e-03  -3.71881900e-03\n",
      "  -1.00519822e-03   4.84643737e-03  -3.53238801e-03   1.22420708e-04\n",
      "  -4.58742678e-03   3.71732470e-03  -4.15829709e-03  -3.93305486e-03\n",
      "  -8.70488118e-04  -4.59041959e-03   3.76840495e-03  -3.98694444e-03\n",
      "   1.56842021e-03   4.36723279e-03   1.75691827e-03   1.42543553e-03\n",
      "  -3.02267703e-03   1.56308757e-03   1.13664242e-03  -1.06631417e-03\n",
      "   4.95949062e-03   1.51077483e-03  -3.55202728e-03  -1.73709460e-03\n",
      "  -4.47266269e-03  -3.67703475e-03   3.55967181e-03  -7.38290721e-04\n",
      "   7.48101273e-04   2.56760861e-03   8.07312434e-04  -2.70612515e-03\n",
      "  -4.23209416e-03   3.43392487e-03   9.29186004e-04   4.30594292e-03\n",
      "  -1.68683392e-03   2.32448475e-03  -3.98656679e-03   4.22142725e-03\n",
      "   3.05290613e-03   2.75243446e-03   4.32301033e-03  -3.94099904e-03\n",
      "  -2.20075296e-03  -2.56885798e-03  -4.12407611e-03  -1.56984746e-03\n",
      "  -3.00061726e-03  -3.19348928e-03   2.11892696e-03  -4.91847843e-03\n",
      "  -2.95917527e-03  -2.72088544e-03  -3.68361338e-03   4.82129958e-03\n",
      "  -3.23147955e-03   7.92523380e-04  -1.13938085e-03   4.13301494e-03\n",
      "  -2.28527421e-03   1.18826900e-03   4.92700469e-03   1.03034917e-03\n",
      "  -2.92705768e-03   2.56462279e-03   3.56607517e-04   3.56026203e-03\n",
      "   7.45267535e-05  -4.00806777e-04  -3.83335561e-03  -1.51982182e-03\n",
      "  -1.27809716e-03  -3.43685434e-03  -3.12654581e-03   2.05130153e-03\n",
      "  -4.02728189e-03  -3.57793015e-03  -1.95463211e-03  -1.00961013e-03\n",
      "   2.47778278e-03  -1.26721070e-03  -3.62945604e-03  -1.76866830e-03]\n",
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "            ['this', 'is', 'the', 'second', 'sentence'],\n",
    "            ['yet', 'another', 'sentence'],\n",
    "            ['one', 'more', 'sentence'],\n",
    "            ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-20 15:38:03,764 : INFO : collecting all words and their counts\n",
      "2017-11-20 15:38:03,766 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-20 15:38:08,538 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-11-20 15:38:08,539 : INFO : Loading a fresh vocabulary\n",
      "2017-11-20 15:38:08,889 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-11-20 15:38:08,890 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-11-20 15:38:09,042 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-11-20 15:38:09,068 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-11-20 15:38:09,069 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-11-20 15:38:09,070 : INFO : estimated required memory for 71290 words and 200 dimensions: 149709000 bytes\n",
      "2017-11-20 15:38:09,295 : INFO : resetting layer weights\n",
      "2017-11-20 15:38:09,967 : INFO : training model with 3 workers on 71290 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-20 15:38:10,990 : INFO : PROGRESS: at 1.25% examples, 758974 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:11,995 : INFO : PROGRESS: at 2.50% examples, 767195 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:13,006 : INFO : PROGRESS: at 3.74% examples, 765892 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:14,013 : INFO : PROGRESS: at 4.94% examples, 761461 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:15,015 : INFO : PROGRESS: at 6.13% examples, 759427 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:16,023 : INFO : PROGRESS: at 7.38% examples, 764284 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:17,028 : INFO : PROGRESS: at 8.63% examples, 766250 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:18,036 : INFO : PROGRESS: at 9.85% examples, 765776 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:19,040 : INFO : PROGRESS: at 11.06% examples, 764853 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:20,043 : INFO : PROGRESS: at 12.26% examples, 763395 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:21,052 : INFO : PROGRESS: at 13.26% examples, 750338 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:22,053 : INFO : PROGRESS: at 14.42% examples, 748086 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:23,062 : INFO : PROGRESS: at 15.63% examples, 747109 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:24,065 : INFO : PROGRESS: at 16.72% examples, 742375 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:25,087 : INFO : PROGRESS: at 17.92% examples, 741914 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:26,088 : INFO : PROGRESS: at 18.94% examples, 735204 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:27,102 : INFO : PROGRESS: at 20.12% examples, 734413 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:28,107 : INFO : PROGRESS: at 21.35% examples, 735834 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:29,114 : INFO : PROGRESS: at 22.60% examples, 737618 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:30,128 : INFO : PROGRESS: at 23.83% examples, 738780 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:31,141 : INFO : PROGRESS: at 25.00% examples, 737970 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:32,145 : INFO : PROGRESS: at 25.97% examples, 732535 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:33,147 : INFO : PROGRESS: at 27.04% examples, 730026 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:34,153 : INFO : PROGRESS: at 27.98% examples, 724067 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:35,158 : INFO : PROGRESS: at 29.09% examples, 722696 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:36,167 : INFO : PROGRESS: at 30.32% examples, 724471 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:37,171 : INFO : PROGRESS: at 31.56% examples, 726258 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:38,184 : INFO : PROGRESS: at 32.79% examples, 727662 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:39,201 : INFO : PROGRESS: at 33.93% examples, 726932 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-20 15:38:40,196 : INFO : PROGRESS: at 35.06% examples, 726115 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-20 15:38:41,207 : INFO : PROGRESS: at 36.17% examples, 724313 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:42,215 : INFO : PROGRESS: at 37.38% examples, 725091 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:43,237 : INFO : PROGRESS: at 38.55% examples, 724908 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:44,252 : INFO : PROGRESS: at 39.81% examples, 726246 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:45,264 : INFO : PROGRESS: at 41.02% examples, 726979 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-20 15:38:46,265 : INFO : PROGRESS: at 42.23% examples, 727328 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:47,270 : INFO : PROGRESS: at 43.47% examples, 728540 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:48,275 : INFO : PROGRESS: at 44.69% examples, 729360 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:49,275 : INFO : PROGRESS: at 45.88% examples, 729884 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:50,283 : INFO : PROGRESS: at 47.10% examples, 730852 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:51,295 : INFO : PROGRESS: at 48.35% examples, 731949 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:52,292 : INFO : PROGRESS: at 49.32% examples, 729117 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:53,295 : INFO : PROGRESS: at 50.50% examples, 729255 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:54,302 : INFO : PROGRESS: at 51.62% examples, 728543 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:55,307 : INFO : PROGRESS: at 52.77% examples, 728361 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:56,316 : INFO : PROGRESS: at 53.85% examples, 727116 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:38:57,318 : INFO : PROGRESS: at 54.97% examples, 726529 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:58,321 : INFO : PROGRESS: at 55.90% examples, 723122 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:38:59,328 : INFO : PROGRESS: at 56.97% examples, 721890 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:00,337 : INFO : PROGRESS: at 58.14% examples, 722051 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:01,337 : INFO : PROGRESS: at 59.21% examples, 720947 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:02,341 : INFO : PROGRESS: at 60.33% examples, 720392 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:03,342 : INFO : PROGRESS: at 61.49% examples, 720363 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:04,346 : INFO : PROGRESS: at 62.65% examples, 720243 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:05,349 : INFO : PROGRESS: at 63.86% examples, 720862 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:06,357 : INFO : PROGRESS: at 65.09% examples, 721750 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:07,362 : INFO : PROGRESS: at 66.29% examples, 722362 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:08,363 : INFO : PROGRESS: at 67.51% examples, 723188 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:09,371 : INFO : PROGRESS: at 68.70% examples, 723448 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:10,376 : INFO : PROGRESS: at 69.94% examples, 724262 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:11,380 : INFO : PROGRESS: at 71.15% examples, 724826 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:12,389 : INFO : PROGRESS: at 72.37% examples, 725389 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:13,389 : INFO : PROGRESS: at 73.53% examples, 725442 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:14,400 : INFO : PROGRESS: at 74.69% examples, 725321 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:15,406 : INFO : PROGRESS: at 75.84% examples, 724909 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:16,413 : INFO : PROGRESS: at 77.05% examples, 725293 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:17,414 : INFO : PROGRESS: at 78.13% examples, 724593 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:18,420 : INFO : PROGRESS: at 79.22% examples, 723924 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:19,415 : INFO : PROGRESS: at 80.29% examples, 723069 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-20 15:39:20,426 : INFO : PROGRESS: at 81.39% examples, 722263 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:21,434 : INFO : PROGRESS: at 82.59% examples, 722524 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:22,436 : INFO : PROGRESS: at 83.81% examples, 723071 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:23,439 : INFO : PROGRESS: at 85.03% examples, 723652 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:24,441 : INFO : PROGRESS: at 86.24% examples, 724239 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:25,449 : INFO : PROGRESS: at 87.40% examples, 724211 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-20 15:39:26,452 : INFO : PROGRESS: at 88.48% examples, 723578 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:27,456 : INFO : PROGRESS: at 89.58% examples, 723160 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:28,457 : INFO : PROGRESS: at 90.78% examples, 723536 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:29,464 : INFO : PROGRESS: at 91.99% examples, 723938 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:30,469 : INFO : PROGRESS: at 93.22% examples, 724410 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:31,471 : INFO : PROGRESS: at 94.40% examples, 724649 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:32,492 : INFO : PROGRESS: at 95.63% examples, 724835 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-20 15:39:33,484 : INFO : PROGRESS: at 96.85% examples, 725293 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:34,506 : INFO : PROGRESS: at 98.01% examples, 725165 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-20 15:39:35,510 : INFO : PROGRESS: at 99.12% examples, 724664 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-20 15:39:36,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-20 15:39:36,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-20 15:39:36,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-20 15:39:36,219 : INFO : training on 85026035 raw words (62534964 effective words) took 86.3s, 725035 effective words/s\n",
      "2017-11-20 15:39:36,221 : INFO : saving Word2Vec object under text8.model, separately None\n",
      "2017-11-20 15:39:36,221 : INFO : not storing attribute syn0norm\n",
      "2017-11-20 15:39:36,223 : INFO : storing np array 'syn0' to text8.model.wv.syn0.npy\n",
      "2017-11-20 15:39:36,254 : INFO : storing np array 'syn1neg' to text8.model.syn1neg.npy\n",
      "2017-11-20 15:39:36,286 : INFO : not storing attribute cum_table\n",
      "2017-11-20 15:39:36,863 : INFO : saved text8.model\n",
      "2017-11-20 15:39:36,864 : INFO : storing 71290x200 projection weights into text.model.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = word2vec.Text8Corpus('text8')\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=200)\n",
    "\n",
    "#save as regular word2vec\n",
    "model.save('text8.model')\n",
    "\n",
    "#save as binary format\n",
    "model.wv.save_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-20 15:50:54,747 : INFO : loading Word2Vec object from text8.model\n",
      "2017-11-20 15:50:55,099 : INFO : loading wv recursively from text8.model.wv.* with mmap=None\n",
      "2017-11-20 15:50:55,100 : INFO : loading syn0 from text8.model.wv.syn0.npy with mmap=None\n",
      "2017-11-20 15:50:55,122 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-20 15:50:55,123 : INFO : loading syn1neg from text8.model.syn1neg.npy with mmap=None\n",
      "2017-11-20 15:50:55,145 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-20 15:50:55,146 : INFO : loaded text8.model\n",
      "2017-11-20 15:50:55,330 : INFO : loading projection weights from text.model.bin\n",
      "2017-11-20 15:50:56,080 : INFO : loaded (71290, 200) matrix from text.model.bin\n",
      "2017-11-20 15:50:56,112 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-20 15:50:56,192 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman + king - man\n",
      "[(u'queen', 0.6729247570037842)]\n",
      "man + queeen - woman\n",
      "[(u'king', 0.615934431552887)]\n",
      "\n",
      "\n",
      "[(u'woman', 0.7026777267456055), (u'girl', 0.6160061359405518), (u'creature', 0.5564690828323364), (u'boy', 0.5352729558944702), (u'person', 0.5331730246543884), (u'stranger', 0.5156446695327759), (u'men', 0.5062394142150879), (u'thief', 0.5054370164871216), (u'evil', 0.5005030632019043), (u'gentleman', 0.492491751909256)]\n",
      "\n",
      "\n",
      "[(u'mother', 0.7593399882316589), (u'wife', 0.6856781244277954), (u'grandmother', 0.674963116645813)]\n",
      "'he' is to 'is' as 'she' is to 'exists'\n",
      "'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "'going' is to 'went' as 'being' is to 'was'\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model = word2vec.Word2Vec.load('text8.model')\n",
    "model1 = word2vec.KeyedVectors.load_word2vec_format('text.model.bin', binary=True)\n",
    "\n",
    "print(\"woman + king - man\")\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "\n",
    "\n",
    "print(\"man + queeen - woman\")\n",
    "print(model.most_similar(positive=['man', 'queen'], negative=['woman'], topn=1))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(model.most_similar(['man']))\n",
    "print(\"\\n\")\n",
    "print(model1.most_similar(['girl', 'father'], ['boy'], topn=3))\n",
    "\n",
    "more_examples = [\"he is she\", \"big bigger bad\", \"going went being\"]\n",
    "\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print(\"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
